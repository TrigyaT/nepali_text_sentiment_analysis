# -*- coding: utf-8 -*-
"""5thSep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12RFnD5hBJp4SZNJST5SiITbtsGS-TIYi
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!ls "/content/drive/MyDrive"
!ls "/content/drive/MyDrive/Sentiment_Analysis_Data"

from google.colab import files
import os
import pandas as pd

# Upload CSV manually
uploaded = files.upload()

# Get the filename
csv_filename = list(uploaded.keys())[0]  # gets uploaded file name

# Load CSV into DataFrame
df_tweet_nepali = pd.read_csv(csv_filename, index_col=[0])
df_tweet_nepali.head()

df_tweet_nepali.head()

print(df_tweet_nepali.columns)

df_tweet_nepali = df_tweet_nepali.reset_index()
print(df_tweet_nepali.columns)
display(df_tweet_nepali.head())

num_negative = (df_tweet_nepali['Label'] == '-1').sum()
print("Number of -1 labels:", num_negative)

# Replace 0 with 3
df_tweet_nepali["Label"] = df_tweet_nepali["Label"].replace('-1','2')
# Check counts to confirm
df_tweet_nepali["Label"].value_counts()

num_negative = (df_tweet_nepali['Label'] == '2').sum()
print("Number of 2 labels:", num_negative)

df_tweet_nepali.info()

df_tweet_nepali.describe()

df_tweet_nepali["Label"] = df_tweet_nepali["Label"].values.astype("str")
df_tweet_nepali["Label"] = df_tweet_nepali["Label"].str.lower()
df_tweet_nepali["Label"].value_counts()

"""
#*This is commented here because, it removes a lot of data points that can be fixed.*#
"""
# Removing all value in "Label" columns that does not contain either 2, 0, or, 1
masks = df_tweet_nepali["Label"].str.contains("^[2|0|1]")
df_draft = df_tweet_nepali[masks]
df_draft["Label"].value_counts()

df_draft = df_draft.loc[df_draft['Label'].isin(["0", "1", "2"])]
df_draft.head()

df_draft["Label"].value_counts()

df_tweet_nepali.loc[df_tweet_nepali["Label"].str.contains("-0--"),"Label"] = "0"
df_tweet_nepali.loc[df_tweet_nepali["Label"].str.contains("2",na=False),"Label"] = "2"

non_int_labels = df_tweet_nepali[
    df_tweet_nepali["Label"].str.contains('pos') |
    df_tweet_nepali["Label"].str.contains('neg') |
    df_tweet_nepali["Label"].str.contains('neu')
]
non_int_labels.loc[non_int_labels['Label'].str.contains('pos',na=False),"Label"] = "1"
non_int_labels.loc[non_int_labels['Label'].str.contains('neu',na=False),"Label"] = "0"
non_int_labels.loc[non_int_labels['Label'].str.contains('neg',na=False),"Label"] = "2"

df_tweet_nepali.loc[non_int_labels.index,"Label"] = non_int_labels["Label"]
final_df = df_tweet_nepali[df_tweet_nepali["Label"].isin(["0","1","2"])]
final_df["Label"].value_counts()

final_df = final_df[["Label", "Tweet"]]
final_df.head()

final_df = final_df[["Label", "Tweet"]]
final_df.head()

masks = final_df["Tweet"].str.contains("[A-Za-z]")
masks.value_counts()

# Renaming columns
final_df = final_df.rename(columns={"Tweet": "Sentences", "Label": "Sentiment"})

final_df["Sentiment"] = final_df["Sentiment"].astype("int32")
final_df["Sentences"] = final_df["Sentences"].astype(str)

import re

# Stop Word
nepali_stopwords = [
    "र", "वा", "तर", "यदि", "भएको", "भए", "छन्", "छु", "छ", "हुन्छ", "हुन्",
    "गरेको", "गर्छ", "गर्न", "तथा", "त्यो", "यस", "यो", "भनेको", "जस्तो",
    "हाम्रो", "मेरो", "तिम्रो", "तपाईंको", "उनको", "म", "तिमी", "तपाईं",
    "ऊ", "उनले", "हामी", "हाम्रा", "हुन", "भएकोछ", "भएका", "भएकोथियो",
    "भएकोछ", "हुनसक्छ", "पनि", "मात्र", "लगायत", "साथै", "अनि", "जुन",
    "के", "कसरी", "कसको", "कहाँ", "कसलाई", "कससँग", "कस्तो", "किन", "केही",
    "सबै", "संग", "भित्र", "बाहिर", "मध्य", "रूप", "जस्तै", "जसले", "जसरी",
    "कुनै", "त्यस्तो", "यस्तो", "योही", "त्यही", "तरि", "जसमा", "जहाँ", "जब"
]

def preprocess_nepali(text):
    text = text.lower()

    text = re.sub(r"[^\u0900-\u097F\s]", "", text)


    tokens = text.split()


    tokens = [t for t in tokens if t not in nepali_stopwords]


    tokens = [re.sub(r"(हरु|हरुमा|हरुको)$", "", t) for t in tokens]

    return " ".join(tokens)

final_df["Sentences"] = final_df["Sentences"].apply(preprocess_nepali)

final_df["Sentences"] = final_df["Sentences"].apply(preprocess_nepali)

final_df.info()

import datasets

final_df = datasets.Dataset.from_pandas(final_df)
final_df = final_df.remove_columns(column_names=['__index_level_0__'])
final_df

final_df.save_to_disk("/content/data/preprocess-data")

import datasets

# Load the dataset you saved
final_df = datasets.Dataset.load_from_disk(
    "/content/data/preprocess-data")

final_df['Sentiment']

# Convert to int32 if not already
final_df = final_df.cast_column("Sentiment", datasets.Value("int32"))

# Prepare Python lists
texts = final_df['Sentences'][:]
labels = final_df['Sentiment'][:]

import torch
from collections import Counter
from sklearn.model_selection import train_test_split

# Use GPU if available
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print("Using device:", device)

import torch
from torch.utils.data import Dataset, DataLoader
from collections import Counter

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

class TorchVectorizer:
    def __init__(self):
        self.vocab = {}

    def build_vocab(self, texts):
        idx = 0
        for text in texts:
            for word in text.split():
                if word not in self.vocab:
                    self.vocab[word] = idx
                    idx += 1
        return self.vocab


class NepaliTweetDataset(Dataset):
    def __init__(self, texts, labels, vectorizer):
        self.texts = texts
        self.labels = labels
        self.vectorizer = vectorizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        x = torch.zeros(len(self.vectorizer.vocab), dtype=torch.float32)
        for word in self.texts[idx].split():
            if word in self.vectorizer.vocab:
                x[self.vectorizer.vocab[word]] = 1
        y = self.labels[idx]
        return x, y


y_mapped = torch.tensor(labels, dtype=torch.long)
y_mapped[y_mapped == -1] = 2
y_mapped[y_mapped == 0] = 0
y_mapped[y_mapped == 1] = 1


vectorizer = TorchVectorizer()
vectorizer.build_vocab(texts)

dataset = NepaliTweetDataset(texts, y_mapped, vectorizer)


dataloader = DataLoader(dataset, batch_size=64, shuffle=True)


for batch_x, batch_y in dataloader:
    batch_x, batch_y = batch_x.to(device), batch_y.to(device)
    print("Batch X:", batch_x.shape)
    print("Batch y:", batch_y.shape)
    break

import torch.nn as nn

class LogisticRegressionPyTorch(nn.Module):
    def __init__(self, input_dim, num_classes):
        super().__init__()
        self.linear = nn.Linear(input_dim, num_classes)

    def forward(self, x):
        return self.linear(x)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
input_dim = len(vectorizer.vocab)
num_classes = 3 #bias

model = LogisticRegressionPyTorch(input_dim, num_classes).to(device)

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from sklearn.metrics import accuracy_score, f1_score

train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# Define batch_size
batch_size = 64  # Or any other suitable batch size

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# --- Model, loss, optimizer ---
input_dim = len(vectorizer.vocab)
num_classes = 3
model = LogisticRegressionPyTorch(input_dim, num_classes).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
epochs = 20

# --- Training loop with validation ---
for epoch in range(epochs):
    # -------- Train --------
    model.train()
    running_loss = 0.0
    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_train_loss = running_loss / len(train_loader)

    # -------- Validation --------
    model.eval()
    val_loss = 0.0
    all_preds, all_labels, all_confidences = [], [], []

    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            val_loss += loss.item()

            # Get probabilities with torch.softmax
            probs = torch.softmax(outputs, dim=1)

            # Predicted class + confidence
            confidences, preds = torch.max(probs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(y_batch.cpu().numpy())
            all_confidences.extend(confidences.cpu().numpy())

    avg_val_loss = val_loss / len(val_loader)
    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average="weighted")
    avg_conf = sum(all_confidences) / len(all_confidences)

    print(f"Epoch [{epoch+1}/{epochs}] "
          f"Train Loss: {avg_train_loss:.4f} | "
          f"Val Loss: {avg_val_loss:.4f} | "
          f"Val Accuracy: {acc:.4f} | "
          f"Val F1 Score: {f1:.4f} | "
          f"Avg Confidence: {avg_conf:.4f}")

import torch

torch.save(model.state_dict(), "/content/sentiment_model.pth")
print("Model saved as sentiment_model.pth")

import json

with open("/content/vectorizer_vocab.json", "w", encoding="utf-8") as f:
    json.dump(vectorizer.vocab, f, ensure_ascii=False)

print("Vectorizer vocabulary saved as vectorizer_vocab.json")

label_map = {"2": "Negative", "0": "Neutral", "1": "Positive"}

with open("/content/label_map.json", "w", encoding="utf-8") as f:
    json.dump(label_map, f, ensure_ascii=False)

print("Label map saved as label_map.json")

from google.colab import files

files.download("/content/sentiment_model.pth")
files.download("/content/vectorizer_vocab.json")
files.download("/content/label_map.json")

import torch
import json
import os

# Create a folder in Drive to store your project files
save_dir = "/content/drive/MyDrive/sentiment_project"
os.makedirs(save_dir, exist_ok=True)

# Save model
model_path = os.path.join(save_dir, "sentiment_model.pth")
torch.save(model.state_dict(), model_path)
print(f"Model saved to {model_path}")

#Save vectorizer vocab
vectorizer_path = os.path.join(save_dir, "vectorizer_vocab.json")
with open(vectorizer_path, "w", encoding="utf-8") as f:
    json.dump(vectorizer.vocab, f, ensure_ascii=False)
print(f"Vectorizer vocabulary saved to {vectorizer_path}")

# Save label map
label_map_path = os.path.join(save_dir, "label_map.json")
# Corrected label map to match the model's output (0: Neutral, 1: Positive, 2: Negative)
label_map = {"2": "Negative", "0": "Neutral", "1": "Positive"}
with open(label_map_path, "w", encoding="utf-8") as f:
    json.dump(label_map, f, ensure_ascii=False)
print(f"Label map saved to {label_map_path}")

import os

save_dir = "/content/drive/MyDrive/sentiment_project"
files = os.listdir(save_dir)
print("Files in folder:", files)

model.eval()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# After training loop finishes
model.eval()
all_preds, all_labels = [], []

with torch.no_grad():
    for X_batch, y_batch in val_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        outputs = model(X_batch)
        _, preds = torch.max(outputs, 1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(y_batch.cpu().numpy())

# Confusion matrix
cm = confusion_matrix(all_labels, all_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap="Blues", values_format="d")
plt.title("Confusion Matrix on Validation Set")
plt.show()

import torch

def classification_report_table(preds, labels, class_names):
    preds = torch.tensor(preds)
    labels = torch.tensor(labels)
    num_classes = len(class_names)

    # Header
    print(f"{'Class':10} | {'Precision':9} | {'Recall':7} | {'F1-Score':9} | {'Support':7}")
    print("-"*55)

    # Metrics per class
    for c in range(num_classes):
        tp = ((preds == c) & (labels == c)).sum().item()
        fp = ((preds == c) & (labels != c)).sum().item()
        fn = ((preds != c) & (labels == c)).sum().item()
        support = (labels == c).sum().item()

        precision = tp / (tp + fp + 1e-8)
        recall = tp / (tp + fn + 1e-8)
        f1 = 2 * precision * recall / (precision + recall + 1e-8)

        print(f"{class_names[c]:10} | "
              f"{precision:9.4f} | "
              f"{recall:7.4f} | "
              f"{f1:9.4f} | "
              f"{support:7}")

# --- Usage ---
class_names = ["Negative", "Neutral", "Positive"]  # Make sure this matches your label_map
classification_report_table(all_preds, all_labels, class_names)